{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "ET5003_NLP_SpamClassifier.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "930vlW5BrOtq"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1vK33e_EqaHgBHcbRV_m38hx6IkG0blK_\" width=\"350\"/>\n",
        "</div> \n",
        "\n",
        "#**Artificial Intelligence - MSc**\n",
        "##ET5003 - MACHINE LEARNING APPLICATIONS \n",
        "\n",
        "###Instructor: Enrique Naredo\n",
        "###ET5003_NLP_SpamClasiffier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQyc5Xgdj7W9"
      },
      "source": [
        "\n",
        "### Spam Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z8j0PtO77Xs"
      },
      "source": [
        "[Spamming](https://en.wikipedia.org/wiki/Spamming) is the use of messaging systems to send multiple unsolicited messages (spam) to large numbers of recipients for the purpose of commercial advertising, for the purpose of non-commercial proselytizing, for any prohibited purpose (especially the fraudulent purpose of phishing), or simply sending the same message over and over to the same user. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_fap_t2DrME"
      },
      "source": [
        "Spam Classification: Deciding whether an email is spam or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg7VCbX77eAA"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k96-GLUGE2ux"
      },
      "source": [
        "# standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxMYIOx1FONV"
      },
      "source": [
        "# Scikit-learn is an open source machine learning library \n",
        "# that supports supervised and unsupervised learning\n",
        "# https://scikit-learn.org/stable/\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLlFHWx4j7W6"
      },
      "source": [
        "# Regular expression operations\n",
        "#https://docs.python.org/3/library/re.html\n",
        "import re \n",
        "\n",
        "# Natural Language Toolkit\n",
        "# https://www.nltk.org/install.html\n",
        "import nltk\n",
        "\n",
        "# Stemming maps different forms of the same word to a common “stem” \n",
        "# https://pypi.org/project/snowballstemmer/\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# https://www.nltk.org/book/ch02.html\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4iXKNAGj7W_"
      },
      "source": [
        "## Step 1: Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NY-KIbRkMMJ"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5akGy2kSj7W_"
      },
      "source": [
        "# path to your (local/cloud) drive \n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Enrique/Data/spam/'\n",
        "\n",
        "# load dataset\n",
        "df = pd.read_csv(path+'spam.csv', encoding='latin-1')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkrYoNn3JHxe"
      },
      "source": [
        "# original dataset\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MODwpwEkJGky"
      },
      "source": [
        "# remove useless features\n",
        "df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis='columns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDvr4HpKGZlN"
      },
      "source": [
        "# v1 -> is the class label: ham, spam\n",
        "# ham -> https://en.wiktionary.org/wiki/ham_e-mail\n",
        "# spam -> https://en.wiktionary.org/wiki/spam#English\n",
        "# v2 -> is the  email\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMyuPYcuj7XB"
      },
      "source": [
        "## Step 2: Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmTe13CNKS_s"
      },
      "source": [
        "# Removing stopwords and stemming\n",
        "# a stem must be a word\n",
        "# Example:  fishing, fished, and fisher: stem -> fish\n",
        "# choose English as the target language\n",
        "stemmer = SnowballStemmer('english', ignore_stopwords=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNG-xh8wMOTi"
      },
      "source": [
        "# Stop words are basically a set of commonly used words in any language\n",
        "# https://en.wikipedia.org/wiki/Stop_word\n",
        "# and are filtered out before processing of natural language data \n",
        "# Example list: https://github.com/igorbrigadir/stopwords/blob/master/en/terrier.txt\n",
        "nltk.download('stopwords')\n",
        "stop = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3mbvmBFO310"
      },
      "source": [
        "# example: remove anything that is not a letter\n",
        "string_sample = '123This @45is 890-130 an_example !!'\n",
        "new_string = re.sub('[^a-zA-Z]', ' ', string_sample) \n",
        "print(new_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riD32a4VSQPb"
      },
      "source": [
        "# removing duplicated spaces\n",
        "\" \".join(new_string.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF15oylkN_UD"
      },
      "source": [
        "# remove anything that is not a letter in the emails\n",
        "df['v2'] = [re.sub('[^a-zA-Z]', ' ', sms) for sms in df['v2']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujye-Z_GNcKm"
      },
      "source": [
        "# list of words in the emails\n",
        "email_words = [sms.split() for sms in df['v2']]\n",
        "print(email_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCzsxUkXj7XB"
      },
      "source": [
        "# function to normalize words\n",
        "def normalize(words):\n",
        "  normalized_words = list()\n",
        "  for word in words:\n",
        "    # remove  the most common words\n",
        "    if word.lower() not in stop: \n",
        "      # stemming\n",
        "      new_word = stemmer.stem(word) \n",
        "      # lower case\n",
        "      normalized_words.append(new_word.lower()) \n",
        "  return normalized_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACEku-N6Tag9"
      },
      "source": [
        "# normalize words in emails\n",
        "email_words_norm = [normalize(word) for word in email_words]\n",
        "print(email_words_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBNa4IlTgUef"
      },
      "source": [
        "# update dataframe\n",
        "df['v2'] = [\" \".join(word) for word in email_words_norm]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUSE99a6j7XC"
      },
      "source": [
        "# training and test datasets\n",
        "data_register = df['v2']\n",
        "class_label = df['v1']\n",
        "factor = 0.2\n",
        "lucky_number = 7\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_register, class_label, test_size=factor, random_state=lucky_number)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7N04_o7iVCe"
      },
      "source": [
        "# train\n",
        "print(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0_UqNlqi62U"
      },
      "source": [
        "# test\n",
        "print(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttq_X1YHjY1p"
      },
      "source": [
        "# class labels in training\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD7KRHWsj7XC"
      },
      "source": [
        "# reshape -> gives a new shape to an array without changing its data\n",
        "# https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.reshape.html#numpy.reshape\n",
        "# -1 -> the unspecified value is inferred\n",
        "y_train.values.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8NL5aZ4fb0T"
      },
      "source": [
        "**Build your custome function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USUzEMJSW5WL"
      },
      "source": [
        "The real difference between stemming and lemmatization is that stemming reduces word-forms to (pseudo) stems which might be meaningful or meaningless, whereas lemmatization reduces the word-forms to linguistically valid meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h5pJ9tNWEJn"
      },
      "source": [
        "# you can build your own NLP function\n",
        "# edit it according to your requirements\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import *\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def NLP_preprocess(some_text):\n",
        "  \"\"\"\n",
        "  Normalization using NLTK and spaCy\n",
        "  \"\"\"\n",
        "  # 1. Tokenization\n",
        "  NLP_token = word_tokenize(some_text)\n",
        "\n",
        "  # 2. Stemming\n",
        "  PS = PorterStemmer()\n",
        "  NLP_stem = []\n",
        "  for word in NLP_token:\n",
        "      NLP_stem.append(PS.stem(word))\n",
        "\n",
        "  # 3. Lemmatization\n",
        "  WL = WordNetLemmatizer()\n",
        "  NLP_lemma = []\n",
        "  for word in NLP_stem:\n",
        "      NLP_lemma.append(WL.lemmatize(word))\n",
        "  \n",
        "  # 4. Stopword   \n",
        "  FS = []  \n",
        "  NLP_stop = set(stopwords.words(\"english\"))\n",
        "  for w in NLP_lemma:  \n",
        "      if w not in NLP_stop:  \n",
        "        FS.append(w)\n",
        "  \n",
        "  # 5. Punctuation  \n",
        "  punctuations = \"?:!.,;\"\n",
        "  for word in FS:\n",
        "      if word in punctuations:\n",
        "          FS.remove(word)\n",
        "\n",
        "  # print comparison\n",
        "  print(\" \")\n",
        "  print(some_text)\n",
        "  print(FS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzxo1AWSWFTz"
      },
      "source": [
        "# example\n",
        "NLP_preprocess(string_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpnCzn7Jj7XD"
      },
      "source": [
        "## Step 3: Counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kcKwXjDJ5ES"
      },
      "source": [
        "**Create new features with NLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWzId1JpmOYt"
      },
      "source": [
        "# Convert a collection of text documents to a matrix of token counts\n",
        "CV = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m30-9q5mwYF"
      },
      "source": [
        "## training\n",
        "# transforming email into counts\n",
        "# counting the number of times a word appears in each email\n",
        "# 'x_train_count' is a sparse matrix, this avoids to store the zeroes\n",
        "x_train_count = CV.fit_transform(x_train)\n",
        "\n",
        "# returns: n_samples, n_features\n",
        "print(\"total emails =\", x_train_count.shape[0])\n",
        "print(\"total words =\", x_train_count.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzOt7nuUrACv"
      },
      "source": [
        "# show the counts in train\n",
        "print(x_train_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrovMutirKZS"
      },
      "source": [
        "# full matrix\n",
        "x_train_count.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yrAimS4j7XD"
      },
      "source": [
        "## test\n",
        "# transforming email into counts\n",
        "# counting the number of times a word appears in each email\n",
        "# using the vocabulary fitted with '.fit'\n",
        "# sparse matrix: only non-zeroes elements are stored\n",
        "x_test_count = CV.transform(x_test)\n",
        "\n",
        "# returns: n_samples, n_features\n",
        "print(\"total emails =\", x_test_count.shape[0])\n",
        "print(\"total words =\", x_test_count.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcxtAqZhrt8h"
      },
      "source": [
        "# array mapping from feature integer indices to feature name\n",
        "int2feature = CV.get_feature_names()\n",
        "print(int2feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd50PJqPv1Np"
      },
      "source": [
        "# warning:\n",
        "# be aware that running several times next cell\n",
        "# it will append 'Class' each time \n",
        "c = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z2mYaDqsgVI"
      },
      "source": [
        "# append 'Class' to the end of the list\n",
        "if c==0:\n",
        "  int2feature.append('Class')\n",
        "  # print last 10 feature names\n",
        "  print(int2feature[len(int2feature)-10:len(int2feature)-1])\n",
        "  c = 1\n",
        "else:\n",
        "  print('already appended')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISTJS6Gvj7XE"
      },
      "source": [
        "# new dataset\n",
        "new_dataset = pd.DataFrame(data=np.hstack([x_train_count.toarray(),y_train.values.reshape(-1,1)]), columns=int2feature)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3QXMdhIypsb"
      },
      "source": [
        "# first rows\n",
        "new_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqk2jRleyxd3"
      },
      "source": [
        "new_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8RoL4Z7uk62"
      },
      "source": [
        "# write object to a comma-separated values (csv) file\n",
        "# verify in your folder\n",
        "new_dataset.to_csv(path+\"spam_clean.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyHnie_5j7XE"
      },
      "source": [
        "### Data format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdrMNE8jj7XF"
      },
      "source": [
        "# select an email number\n",
        "row_index = 0\n",
        "print(\"non-sparse matrix =\", x_train_count[row_index,:].todense())\n",
        "\n",
        "# original words in the email\n",
        "print('original words: ', x_train.values[row_index])\n",
        "\n",
        "# decoded numerical input \n",
        "DNI = x_train_count[row_index,:].todense()\n",
        "# inverse_transform: return terms per document with nonzero entries\n",
        "print('decoded input: ', CV.inverse_transform(DNI))\n",
        "\n",
        "# index of words\n",
        "ind = np.where(DNI[0,:]>0)[1]\n",
        "print('word indexes: ', ind)\n",
        "\n",
        "# number of times those words appears in the email\n",
        "print(x_train_count[row_index,ind].todense())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGilAvtbj7XF"
      },
      "source": [
        "## Step 4: Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-KYhFh859c-"
      },
      "source": [
        "Training the classifier and making predictions on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9ZrtUKej7XF"
      },
      "source": [
        "# create a model\n",
        "MNB = MultinomialNB()\n",
        "\n",
        "# fit to data\n",
        "MNB.fit(x_train_count, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW7hoVB76Yqf"
      },
      "source": [
        "# testing the model\n",
        "\n",
        "prediction_train = MNB.predict(x_train_count)\n",
        "print('training prediction\\t', prediction_train)\n",
        "\n",
        "prediction_test = MNB.predict(x_test_count)\n",
        "print('test prediction\\t\\t', prediction_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcAd5Wh6j7XG"
      },
      "source": [
        "# set_printoptions: If True, always print floating point numbers \n",
        "# using fixed point notation, in which case numbers equal to zero \n",
        "# in the current precision will print as zero\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Ham and Spam probabilities in test\n",
        "class_prob = MNB.predict_proba(x_test_count)\n",
        "print(class_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqFxSVTzj7XG"
      },
      "source": [
        "# show emails classified as 'spam'\n",
        "threshold = 0.5\n",
        "spam_ind = np.where(class_prob[:,1]>threshold)[0]\n",
        "print(x_test.values[spam_ind])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkIb-_wUj7XG"
      },
      "source": [
        "## Step 5: Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjreL3sdj7XH"
      },
      "source": [
        "# accuracy in training set\n",
        "y_pred_train = prediction_train\n",
        "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MQRTS8rTj7XH"
      },
      "source": [
        "# accuracy in test set (unseen data)\n",
        "y_true = y_test\n",
        "y_pred_test = prediction_test\n",
        "print(\"Test Accuracy: \"+str(accuracy_score(y_true, y_pred_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqg3jqtWAPJB"
      },
      "source": [
        "# confusion matrix\n",
        "conf_mat = confusion_matrix(y_true, y_pred_test)\n",
        "print(\"Confusion Matrix\\n\", conf_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypc8HXCYEpNz"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "labels = ['Ham','Spam']\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(conf_mat)\n",
        "plt.title('Confusion matrix of the classifier\\n')\n",
        "fig.colorbar(cax)\n",
        "ax.set_xticklabels([''] + labels)\n",
        "ax.set_yticklabels([''] + labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}